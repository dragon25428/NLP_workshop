{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "417880eb038c49c346aaca12af5a57d5875e15532e722506ed6731d77d3493bc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Practice 5 - Bag-of-N-grams models\n",
    "### Strictly used for internal purpose in Singapore Polytechnic. Do not disclose!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "One hot encoding, BoW and TF-IDF treat words as independent units. There is no notion of phrases or word ordering. Bag of Ngrams (BoN) approach tries to remedy this. It does so by breaking text into chunks of n countigous words/tokens. This can help us capture some context, which earlier approaches could not do. Let us see how it works using the same toy corpus we used in earlier examples."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['students are learning nlp',\n",
       " 'nlp workshop is interesting, students like nlp',\n",
       " 'students are studying math',\n",
       " 'math is foundation of nlp']"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "#our corpus\n",
    "documents = [\"Students are learning NLP.\",\n",
    "             \"NLP workshop is interesting, students like NLP\",\n",
    "             \"Students are studying math\",\n",
    "             \"Math is foundation of NLP\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Our vocabulary:  [('are', 0), ('are learning', 1), ('are studying', 2), ('foundation', 3), ('foundation of', 4), ('interesting', 5), ('interesting students', 6), ('is', 7), ('is foundation', 8), ('is interesting', 9), ('learning', 10), ('learning nlp', 11), ('like', 12), ('like nlp', 13), ('math', 14), ('math is', 15), ('nlp', 16), ('nlp workshop', 17), ('of', 18), ('of nlp', 19), ('students', 20), ('students are', 21), ('students like', 22), ('studying', 23), ('studying math', 24), ('workshop', 25), ('workshop is', 26)]\n"
     ]
    }
   ],
   "source": [
    "#Ngram vectorization example with count vectorizer and uni, bi, trigrams\n",
    "count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", sorted(count_vect.vocabulary_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BoW representation for 'Students are learning NLP.':  [[1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0]]\nBoW representation for 'NLP workshop is interesting, students like NLP':  [[0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 2 1 0 0 1 0 1 0 0 1 1]]\nBow representation for 'nlp is harder than math, but students like nlp': [[0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 2 0 0 0 1 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#see the BOW rep for first 2 documents\n",
    "print(f\"BoW representation for '{documents[0]}': \", bow_rep[0].toarray())\n",
    "print(f\"BoW representation for '{documents[1]}': \",bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"nlp is harder than math, but students like nlp\"])\n",
    "\n",
    "print(\"Bow representation for 'nlp is harder than math, but students like nlp':\", temp.toarray())"
   ]
  },
  {
   "source": [
    "### Note that the number of features (and hence the size of the feature vector) increased a lot for the same data, compared to the ther single word based representations!!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}