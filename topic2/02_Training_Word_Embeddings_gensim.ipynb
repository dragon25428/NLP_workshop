{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "417880eb038c49c346aaca12af5a57d5875e15532e722506ed6731d77d3493bc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Practical 2. Training Embeddings Using Gensim\n",
    "### Strictly used for internal purpose in Singapore Polytechnic. Do not disclose!\n",
    "Word embeddings are an approach to representing text in NLP. In this notebook we will demonstrate how to train embeddings using Genism. Gensim is an open source Python library for natural language processing, with a focus on topic modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training data\n",
    "#Genism word2vec requires that a format of ‘list of lists’ be provided for training where every document contained in a list.\n",
    "#Every list contains lists of tokens of that document.\n",
    "corpus = [['students', 'learn', 'nlp'],\n",
    "             ['nlp', 'workshop', 'interesting', 'students', 'like', 'nlp'],\n",
    "             ['students', 'study', 'math'],\n",
    "             ['math', 'foundation', 'nlp']]\n",
    "\n",
    "#Training the model\n",
    "model_cbow = Word2Vec(corpus, min_count=1,sg=0) #using CBOW Architecture for trainnig\n",
    "model_skipgram = Word2Vec(corpus, min_count=1,sg=1)#using skipGram Architecture for training"
   ]
  },
  {
   "source": [
    "## Continuous Bag of Words (CBOW)\n",
    "In CBOW, the primary task is to build a language model that correctly predicts the center word given the context words in which the center word appears."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2Vec(vocab=9, size=100, alpha=0.025)\n\nvectors dimension: 100\n\n ['students', 'learn', 'nlp', 'workshop', 'interesting', 'like', 'study', 'math', 'foundation']\n\n [-4.7427327e-03  2.7009989e-03  2.4552299e-03  1.8800957e-03\n -4.0438152e-03  3.4139876e-03  4.7604516e-03  1.1196997e-03\n -3.0536046e-03  6.1592931e-04  2.1515526e-03 -3.3468381e-03\n -2.3813201e-03 -8.1891289e-05 -4.7015934e-03 -2.3964678e-03\n  3.3518907e-03  3.6684147e-04  3.3118124e-03  4.1195285e-03\n  1.9303175e-04 -3.1120996e-03  3.7053400e-03  2.5978107e-03\n -4.4858209e-03  3.9598527e-03 -3.5895035e-03  2.1809381e-03\n -3.7893979e-03  2.7558657e-03  6.0624594e-04 -1.1191514e-03\n  3.0170311e-03  4.4734483e-03 -4.6698484e-04 -2.0793968e-04\n -1.4159449e-03  6.1119179e-04  6.5389578e-04 -8.5016299e-04\n -3.8867856e-03  1.3034858e-04 -9.3512301e-04 -1.1513102e-03\n  3.2631170e-03 -3.1818855e-03  3.6724450e-03  4.7610798e-03\n  4.0262188e-03 -2.9033674e-03  2.7980015e-04  6.5813999e-04\n  4.1850712e-03  3.6857466e-03 -4.7332514e-03 -2.0061331e-04\n  6.1829406e-04 -3.1011638e-03  4.2272755e-03 -4.6886974e-03\n  3.6550260e-03  3.0039239e-04 -8.4939296e-04  2.8267072e-04\n  2.9418457e-04 -3.6089509e-03 -3.3389041e-03 -5.9254077e-04\n  1.3065900e-04  2.9068878e-03 -2.7675319e-03  5.3108868e-04\n  3.7245776e-03 -9.6176274e-04  3.0817222e-03 -1.3329152e-03\n -2.8250834e-03  2.9367006e-03  8.9066307e-04 -1.8364416e-03\n  3.7604542e-03 -2.0296152e-03  3.3568458e-03 -2.3948424e-03\n -9.6200546e-04  2.7756398e-03 -1.1073898e-03 -4.9960688e-03\n  4.9476875e-03  2.9497505e-03 -2.2806891e-03 -3.4374378e-03\n -4.4260086e-03 -4.5706942e-03 -3.8881782e-03  1.9386328e-03\n  4.5771785e-03  2.3797166e-03 -8.1366283e-04  2.7074281e-03]\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(model_cbow)\n",
    "\n",
    "# Summarize vector dimension\n",
    "print(f'\\nvectors dimension: {model_cbow.vector_size}')\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(model_cbow.wv.vocab)\n",
    "print(f'\\n {words}')\n",
    "\n",
    "#Acess vector for one word\n",
    "print(f\"\\n {model_cbow['math']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Similarity between math and nlp: 0.09816745\nSimilarity between math and foundation: 0.034215108\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity \n",
    "print(\"Similarity between math and nlp:\",model_cbow.similarity('math', 'nlp'))\n",
    "print(\"Similarity between math and foundation:\",model_cbow.similarity('math', 'foundation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('workshop', 0.28242602944374084),\n",
       " ('students', 0.23566681146621704),\n",
       " ('nlp', 0.09816744923591614),\n",
       " ('like', 0.09050403535366058),\n",
       " ('learn', 0.04797886312007904),\n",
       " ('foundation', 0.034215107560157776),\n",
       " ('interesting', 0.021503645926713943),\n",
       " ('study', 0.01100192591547966)]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Most similarity\n",
    "model_cbow.most_similar('math')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2Vec(vocab=9, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_cbow.save('model/model_cbow.bin')\n",
    "\n",
    "# load model\n",
    "new_model_cbow = Word2Vec.load('model/model_cbow.bin')\n",
    "print(new_model_cbow)"
   ]
  },
  {
   "source": [
    "## SkipGram\n",
    "In skipgram, the task is to predict the context words from the center word."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2Vec(vocab=9, size=100, alpha=0.025)\n\nvectors dimension: 100\n\n ['students', 'learn', 'nlp', 'workshop', 'interesting', 'like', 'study', 'math', 'foundation']\n\n [-4.7427327e-03  2.7009989e-03  2.4552299e-03  1.8800957e-03\n -4.0438152e-03  3.4139876e-03  4.7604516e-03  1.1196997e-03\n -3.0536046e-03  6.1592931e-04  2.1515526e-03 -3.3468381e-03\n -2.3813201e-03 -8.1891289e-05 -4.7015934e-03 -2.3964678e-03\n  3.3518907e-03  3.6684147e-04  3.3118124e-03  4.1195285e-03\n  1.9303175e-04 -3.1120996e-03  3.7053400e-03  2.5978107e-03\n -4.4858209e-03  3.9598527e-03 -3.5895035e-03  2.1809381e-03\n -3.7893979e-03  2.7558657e-03  6.0624594e-04 -1.1191514e-03\n  3.0170311e-03  4.4734483e-03 -4.6698484e-04 -2.0793968e-04\n -1.4159449e-03  6.1119179e-04  6.5389578e-04 -8.5016299e-04\n -3.8867856e-03  1.3034858e-04 -9.3512301e-04 -1.1513102e-03\n  3.2631170e-03 -3.1818855e-03  3.6724450e-03  4.7610798e-03\n  4.0262188e-03 -2.9033674e-03  2.7980015e-04  6.5813999e-04\n  4.1850712e-03  3.6857466e-03 -4.7332514e-03 -2.0061331e-04\n  6.1829406e-04 -3.1011638e-03  4.2272755e-03 -4.6886974e-03\n  3.6550260e-03  3.0039239e-04 -8.4939296e-04  2.8267072e-04\n  2.9418457e-04 -3.6089509e-03 -3.3389041e-03 -5.9254077e-04\n  1.3065900e-04  2.9068878e-03 -2.7675319e-03  5.3108868e-04\n  3.7245776e-03 -9.6176274e-04  3.0817222e-03 -1.3329152e-03\n -2.8250834e-03  2.9367006e-03  8.9066307e-04 -1.8364416e-03\n  3.7604542e-03 -2.0296152e-03  3.3568458e-03 -2.3948424e-03\n -9.6200546e-04  2.7756398e-03 -1.1073898e-03 -4.9960688e-03\n  4.9476875e-03  2.9497505e-03 -2.2806891e-03 -3.4374378e-03\n -4.4260086e-03 -4.5706942e-03 -3.8881782e-03  1.9386328e-03\n  4.5771785e-03  2.3797166e-03 -8.1366283e-04  2.7074281e-03]\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(model_skipgram)\n",
    "\n",
    "# Summarize vector dimension\n",
    "print(f'\\nvectors dimension: {model_skipgram.vector_size}')\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(model_skipgram.wv.vocab)\n",
    "print(f'\\n {words}')\n",
    "\n",
    "#Acess vector for one word\n",
    "print(f\"\\n {model_skipgram['math']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Similarity between math and nlp: 0.09816745\nSimilarity between math and foundation: 0.034215108\n"
     ]
    }
   ],
   "source": [
    "#Compute similarity \n",
    "print(\"Similarity between math and nlp:\",model_skipgram.similarity('math', 'nlp'))\n",
    "print(\"Similarity between math and foundation:\",model_skipgram.similarity('math', 'foundation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('workshop', 0.28242602944374084),\n",
       " ('students', 0.23566681146621704),\n",
       " ('nlp', 0.09816744923591614),\n",
       " ('like', 0.09050403535366058),\n",
       " ('learn', 0.04797886312007904),\n",
       " ('foundation', 0.034215107560157776),\n",
       " ('interesting', 0.021503645926713943),\n",
       " ('study', 0.01100192591547966)]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Most similarity\n",
    "model_skipgram.most_similar('math')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2Vec(vocab=9, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_skipgram.save('model/model_skipgram.bin')\n",
    "\n",
    "# load model\n",
    "new_model_skipgram = Word2Vec.load('model/model_skipgram.bin')\n",
    "print(model_skipgram)"
   ]
  },
  {
   "source": [
    "## Training Word Embedding on Wiki Corpus\n",
    "The corpus download page : https://www.corpusdata.org/formats.asp\n",
    "The entire wiki corpus as of 28/04/2020 is just over 16GB in size. We will take a part of this corpus due to computation constraints and train our word2vec and fasttext embeddings."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open('data/wiki.txt', 'r')\n",
    "corpus = text_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "corpus = corpus.lower()\n",
    "corpus = re.sub(r'\\d','', corpus)\n",
    "corpus = corpus.split('.')\n",
    "corpus = [re.sub(r'\\d','', i) for i in corpus]\n",
    "corpus = [i.translate(str.maketrans('', '', string.punctuation)) for i in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [i.split() for i in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CBOW Model Training Complete.\nTime taken for training is:9.39 s \n"
     ]
    }
   ],
   "source": [
    "#CBOW\n",
    "start = time.time()\n",
    "word2vec_cbow = Word2Vec(corpus,min_count=10, sg=0)\n",
    "end = time.time()\n",
    "print(\"CBOW Model Training Complete.\\nTime taken for training is:{:.2f} s \".format((end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SkipGram Model Training Complete.\nTime taken for training is:20.50 s \n"
     ]
    }
   ],
   "source": [
    "# Skipgram\n",
    "start = time.time()\n",
    "word2vec_skipgram = Word2Vec(corpus,min_count=10, sg=1)\n",
    "end = time.time()\n",
    "print(\"SkipGram Model Training Complete.\\nTime taken for training is:{:.2f} s \".format((end-start)))"
   ]
  },
  {
   "source": [
    "### An interesting obeseravtion if you noticed is that CBOW trains faster than SkipGram in both cases."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2Vec(vocab=13588, size=100, alpha=0.025)\n--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Summarize the CBOW model\n",
    "print(word2vec_cbow)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['text', 'albert', 'of', 'prussia', 'may', 'march', 'was', 'the', 'last', 'grand', 'master', 'teutonic', 'knights', 'who', 'after', 'converting', 'to', 'became', 'first', 'monarch', 'duchy', 'state', 'that', 'emerged', 'from', 'former', 'monastic', 'european', 'ruler', 'establish', 'as', 'official', 'religion', 'his', 'lands', 'he', 'proved', 'instrumental', 'in', 'political', 'spread', 'its', 'early', 'stage', 'ruling', 'prussian', 'for', 'nearly', 'six', 'decades', 'a', 'member', 'branch', 'house', 'hohenzollern', 's', 'election', 'had', 'brought', 'about', 'hopes', 'fortune', 'skilled', 'administrator', 'and', 'leader', 'did', 'indeed', 'reverse', 'decline', 'order', 'however', 'demands', 'martin', 'luther', 'rebelled', 'against', 'catholic', 'church', 'holy', 'roman', 'empire', 'by', 'into', 'protestant', 'hereditary', 'realm', 'uncle', 'king', 'poland', 'arrangement', 'confirmed', 'treaty', 'krakw', 'pledged', 'personal', 'oath', 'return', 'invested', 'with']\n----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Summarize vocabulary\n",
    "words = list(word2vec_cbow.wv.vocab)\n",
    "print(words[:100])\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.738786    0.8861788  -0.31611848  0.5307762  -1.5027008   1.3605808\n -1.8534917  -0.26915264 -1.0699195   0.22573121  0.34850603  0.0945635\n -0.41275886 -0.09613997 -0.33295634  1.0319223  -0.01559319  1.1972445\n -1.0469635   0.20529337 -0.01645879  0.24352127 -0.32144377  0.02693684\n -0.14583959 -0.39322737  0.6679208  -0.57211834  0.61633205  0.18112165\n -0.26600587  0.38309324  0.08289367  0.53765285 -0.37124366 -0.7403174\n -0.06275565 -0.08273292  0.7806035   0.04157685  0.7516202  -0.06333906\n  0.58929735  0.28825414  0.05223166  0.29272842 -0.59094536  0.68880457\n -0.1688067   0.36569107 -0.89279836  1.0106404   0.29204327 -0.71944004\n -0.3171779   0.25453657 -1.3759314   0.26642907 -1.0476274   0.906037\n  0.36368522 -0.47202477 -0.6266167   0.72990084  0.88393855  0.6026101\n  0.34305233  0.4921666   0.11134485  0.6547872   0.5056012  -0.4189302\n -0.13545096 -0.18129024 -0.33972195 -0.4294363   0.6499062  -0.7497354\n  0.03588779  0.14677809  0.0846085  -0.03514655  1.5866584  -0.05130162\n -0.05679293  0.10377932  0.0873511   0.07318749 -1.0604126   0.746037\n  0.25712776 -0.05991274 -0.92070276 -0.28738463  0.25883    -0.9500266\n  0.39767763  0.14974073  0.9456195   0.77701503]\n----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Acess vector for one word\n",
    "print(word2vec_cbow['science'])\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Similarity between science and engineering: 0.9046574\nSimilarity between computer and movie: 0.09772175\n--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Compute similarity \n",
    "print(\"Similarity between science and engineering:\",word2vec_cbow.similarity('science', 'engineering'))\n",
    "print(\"Similarity between computer and movie:\",word2vec_cbow.similarity('science', 'movie'))\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "word2vec_cbow.save('model/wiki_cbow.bin')\n",
    "word2vec_skipgram.save('model/wiki_skipgram.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}