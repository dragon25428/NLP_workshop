{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "417880eb038c49c346aaca12af5a57d5875e15532e722506ed6731d77d3493bc"
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical 3. Text Classification with Convolutional Neural Network\n",
    "### Strictly used for internal purpose in Singapore Polytechnic. Do not disclose!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate different text classification models trained using the Twitter dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the  data and explore.\n",
    "path = \"data/Sentiment and Emotion in Text/train_data.csv\"\n",
    "data = pd.read_csv(path)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 2 categories and leave out the rest.\n",
    "shortlist = ['sadness', \"happiness\"]\n",
    "data = data[data['sentiment'].isin(shortlist)]\n",
    "labels_index = {'sadness':0, 'happiness':1} \n",
    "data['sentiment'] = data['sentiment'].map(labels_index)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data['content'], data['sentiment'], test_size=0.2, random_state=1234)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training sample size: {len(X_train)}')\n",
    "print(f'Validation sample size: {len(X_val)}')\n",
    "print(f'Testing sample size: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters setting\n",
    "MAX_SEQUENCE = 500   # maximum sentence length\n",
    "MAX_WORDS = 10000   # maximum vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\n",
    "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data.\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train) #Converting text to a vector of word indexes\n",
    "val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting this to sequences to be fed into neural network. max seq. len is the maximum length of sentence\n",
    " #initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
    "X_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE)\n",
    "X_val = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE)\n",
    "X_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE)\n",
    "Y_train = to_categorical(np.asarray(Y_train))\n",
    "Y_val = to_categorical(np.asarray(Y_val))\n",
    "Y_test = to_categorical(np.asarray(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D CNN Model with training our own embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 1D CNN model., training our embedding layer\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Embedding(input_dim=MAX_WORDS, input_length=MAX_SEQUENCE, output_dim=128))\n",
    "cnnmodel.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(pool_size=5))\n",
    "cnnmodel.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(pool_size=5))\n",
    "cnnmodel.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(units=128, activation='relu'))\n",
    "cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                  beta_1=0.9, \n",
    "                                  beta_2=0.999,\n",
    "                                  epsilon=1e-8)\n",
    "                                  \n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A: Why Adam is a popular choice for NLP problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnnmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Train the model. Tune to validation set. \n",
    "cnnmodel.fit(X_train, Y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10, validation_data=(X_val, Y_val))\n",
    "\n",
    "#Evaluate on test set:\n",
    "score, acc = cnnmodel.evaluate(X_test, Y_test)\n",
    "print('Test accuracy with CNN:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D CNN Model with pre-trained embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading embedding matrix.')\n",
    "\n",
    "# first, build index mapping words in the embeddings set to their embedding vector\n",
    "embeddings_index = {}\n",
    "with open('glove.6B/glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f'Load {len(embeddings_index)} word vectors in Glove embeddings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_index[\"family\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "EMBEDDING_DIM = 50  # dimension for the pre-trained word embedding\n",
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load these pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(MAX_WORDS,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE,\n",
    "                            trainable=False)\n",
    "print(\"Preparing of embedding matrix is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 1D CNN model\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(embedding_layer)\n",
    "cnnmodel.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(pool_size=5))\n",
    "cnnmodel.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(pool_size=5))\n",
    "cnnmodel.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(units=128, activation='relu'))\n",
    "cnnmodel.add(Dense(len(labels_index), activation='softmax'))\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "#Train the model. Tune to validation set. \n",
    "cnnmodel.fit(X_train, Y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10, validation_data=(X_val, Y_val))\n",
    "\n",
    "#Evaluate on test set:\n",
    "score, acc = cnnmodel.evaluate(X_test, Y_test)\n",
    "print('Test accuracy with CNN:', acc)"
   ]
  }
 ]
}