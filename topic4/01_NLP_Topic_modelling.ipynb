{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "417880eb038c49c346aaca12af5a57d5875e15532e722506ed6731d77d3493bc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Practical 1. Topic Modelling using LDA\n",
    "### Strictly used for internal purpose in Singapore Polytechnic. Do not disclose!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "source": [
    "### As you can see that there are some distinct themes in the news categories like sports, religion, science, technology, politics etc"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Lets look at some sample news\n",
    "newsgroups_train.data[0]"
   ]
  },
  {
   "source": [
    "## Step 2: Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Wilson\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in newsgroups_train.data:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "print(processed_docs[0])"
   ]
  },
  {
   "source": [
    "## Step 3: Bag of words on the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word 18 (\"rest\") appears 1 time.\nWord 166 (\"clear\") appears 1 time.\nWord 336 (\"refer\") appears 1 time.\nWord 350 (\"true\") appears 1 time.\nWord 391 (\"technolog\") appears 1 time.\nWord 437 (\"christian\") appears 1 time.\nWord 453 (\"exampl\") appears 1 time.\nWord 476 (\"jew\") appears 1 time.\nWord 480 (\"lead\") appears 1 time.\nWord 482 (\"littl\") appears 3 time.\nWord 520 (\"wors\") appears 2 time.\nWord 721 (\"keith\") appears 3 time.\nWord 732 (\"punish\") appears 1 time.\nWord 803 (\"california\") appears 1 time.\nWord 859 (\"institut\") appears 1 time.\nWord 917 (\"similar\") appears 1 time.\nWord 990 (\"allan\") appears 1 time.\nWord 991 (\"anti\") appears 1 time.\nWord 992 (\"arriv\") appears 1 time.\nWord 993 (\"austria\") appears 1 time.\nWord 994 (\"caltech\") appears 2 time.\nWord 995 (\"distinguish\") appears 1 time.\nWord 996 (\"german\") appears 1 time.\nWord 997 (\"germani\") appears 3 time.\nWord 998 (\"hitler\") appears 1 time.\nWord 999 (\"livesey\") appears 2 time.\nWord 1000 (\"motto\") appears 2 time.\nWord 1001 (\"order\") appears 1 time.\nWord 1002 (\"pasadena\") appears 1 time.\nWord 1003 (\"pompous\") appears 1 time.\nWord 1004 (\"popul\") appears 1 time.\nWord 1005 (\"rank\") appears 1 time.\nWord 1006 (\"schneider\") appears 1 time.\nWord 1007 (\"semit\") appears 1 time.\nWord 1008 (\"social\") appears 1 time.\nWord 1009 (\"solntz\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "source": [
    "## Step 4: Running LDA using Bag of Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1. **num_topics** is the number of requested latent topics to be extracted from the training corpus.\n",
    "2. **id2word** is a mapping from word ids (integers) to words (strings).\n",
    "3. **passes** is the number of training passes through the corpus.\n",
    "4. **workers** is the number of extra processes to use for parallelization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 8, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 0 \nWords: 0.012*\"armenian\" + 0.010*\"israel\" + 0.008*\"isra\" + 0.006*\"turkish\" + 0.006*\"kill\" + 0.006*\"jew\" + 0.005*\"arab\" + 0.005*\"govern\" + 0.004*\"live\" + 0.004*\"attack\"\n\n\nTopic: 1 \nWords: 0.009*\"govern\" + 0.006*\"presid\" + 0.005*\"american\" + 0.005*\"weapon\" + 0.005*\"clinton\" + 0.004*\"gun\" + 0.004*\"crime\" + 0.004*\"firearm\" + 0.003*\"public\" + 0.003*\"polic\"\n\n\nTopic: 2 \nWords: 0.022*\"window\" + 0.012*\"file\" + 0.009*\"program\" + 0.007*\"version\" + 0.006*\"graphic\" + 0.006*\"color\" + 0.006*\"imag\" + 0.006*\"display\" + 0.006*\"server\" + 0.005*\"applic\"\n\n\nTopic: 3 \nWords: 0.005*\"pitt\" + 0.005*\"food\" + 0.004*\"bank\" + 0.004*\"medic\" + 0.004*\"research\" + 0.004*\"scienc\" + 0.004*\"health\" + 0.004*\"studi\" + 0.004*\"program\" + 0.004*\"entri\"\n\n\nTopic: 4 \nWords: 0.014*\"game\" + 0.011*\"team\" + 0.009*\"play\" + 0.007*\"player\" + 0.006*\"bike\" + 0.005*\"hockey\" + 0.005*\"season\" + 0.004*\"score\" + 0.004*\"leagu\" + 0.003*\"john\"\n\n\nTopic: 5 \nWords: 0.012*\"drive\" + 0.009*\"chip\" + 0.008*\"card\" + 0.008*\"file\" + 0.008*\"encrypt\" + 0.006*\"data\" + 0.006*\"disk\" + 0.006*\"scsi\" + 0.005*\"secur\" + 0.005*\"clipper\"\n\n\nTopic: 6 \nWords: 0.012*\"christian\" + 0.008*\"jesus\" + 0.006*\"exist\" + 0.005*\"moral\" + 0.005*\"bibl\" + 0.005*\"word\" + 0.005*\"religion\" + 0.005*\"church\" + 0.004*\"life\" + 0.004*\"claim\"\n\n\nTopic: 7 \nWords: 0.013*\"space\" + 0.009*\"nasa\" + 0.006*\"power\" + 0.005*\"wire\" + 0.005*\"orbit\" + 0.004*\"engin\" + 0.004*\"launch\" + 0.003*\"research\" + 0.003*\"technolog\" + 0.003*\"earth\"\n\n\n"
     ]
    }
   ],
   "source": [
    "# For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "source": [
    "Classification of the topics\n",
    "Using the words in each topic and their corresponding weights, there are the categories we can probably infer:\n",
    "\n",
    "0: politics\n",
    "1: gun violence\n",
    "2: graphic card\n",
    "3: food and health\n",
    "4: sports\n",
    "5: encription\n",
    "6: religion\n",
    "7: space"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Step 5: Testing on unseen document"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "From: mccall@mksol.dseg.ti.com (fred j mccall 575-3539)\nSubject: Re: Vandalizing the sky.\nOrganization: Texas Instruments Inc\nLines: 65\n\nIn <C63nA8.4C1@news.cso.uiuc.edu> gfk39017@uxa.cso.uiuc.edu (George F. Krumins) writes:\n\n>I was suggesting that the minority of professional and amateur astronomers\n>have the right to a dark, uncluttered night sky.\n\nAnd from whence does this right stem, that it overrides the 'rights'\nof the rest of us?\n\n>Let me give you an example.  When you watch TV, they have commercials to pay\n>for the programming.  You accept that as part of watching.  If you don't like\n>it, you can turn it off.  If you want to view the night sky, and there is a\n>floating billboard out there, you can't turn it off.  It's the same \n>reasoning that limits billboards in scenic areas.\n\nAnd if you want to view that television station, you have to watch the\ncommercials.  You can't turn them off and still be viewing the\ntelevision station.  In other words, if you don't like what you see,\ndon't look.  There is no 'right' I can think of that you have to force\nother people to conform to your idea of aesthetic behaviour.  What's\nnext, laws regulating how people must dress and look so as to appeal\nto your fashion sense, since you have this 'right' of an aesthetic\nview? \n\n> Pat writes:\n>George.\n\n>\tIt's called a democracy.  The majority rules.  sorry.\n>If ytou don't like it, I suggest you modify the constitution to include\n>a constitutional right to Dark Skies.   The theory of government\n>here is that the majority rules,  except in the nature of fundamental\n>civil rights.\n\n>I say: \n>\tAny reasonably in-depth perusal of American history will show\n>\tyou that many WASPs have continued the practices of prejudice,\n>\tdiscrimination, and violence against others of different\n>\traces, religions, and beliefs, despite the law.\n\nWhich has what to do with the topic of discussion?\n\n>Pat says:\n>If you really are annoyed,   get some legislation\n>to create a dark sky zone,  where in all light emissions are protected\n>in the zone.  Kind of like the national radio quiet zone.  Did you\n>know about that?  near teh Radio telescope  observatory in West virginia,\n>they have a 90?????? mile EMCON zone.  Theoretically they can prevent\n>you from running light AC motors, like air conditioners and Vacuums.\n>In practice, they use it mostly to  control  large radio users.\n\n>I say:\n>What I'm objecting to here is a floating billboard that, presumably,\n>would move around in the sky.  I, for one, am against legislating\n>at all.  I just wish that people had a bit of common courtesy, and\n>would consider how their greed for money impacts the more ethereal and\n>aesthetic values that make us human.  This includes the need for wild\n>and unspoiled things, including the night sky.\n\nOh, I see.  You don't want any legislation that might impinge on you;\nyou just want everyone else on the planet to do what you want.\n\n-- \n\"Insisting on perfect safety is for people who don't have the balls to live\n in the real world.\"   -- Mary Shafer, NASA Ames Dryden\n------------------------------------------------------------------------------\nFred.McCall@dseg.ti.com - I don't speak for others and they don't speak for me.\n\n"
     ]
    }
   ],
   "source": [
    "num = 600\n",
    "unseen_document = newsgroups_test.data[num]\n",
    "print(unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score: 0.4081532657146454\t Topic: 0.013*\"space\" + 0.009*\"nasa\" + 0.006*\"power\" + 0.005*\"wire\" + 0.005*\"orbit\"\nScore: 0.2321239560842514\t Topic: 0.012*\"christian\" + 0.008*\"jesus\" + 0.006*\"exist\" + 0.005*\"moral\" + 0.005*\"bibl\"\nScore: 0.22693298757076263\t Topic: 0.009*\"govern\" + 0.006*\"presid\" + 0.005*\"american\" + 0.005*\"weapon\" + 0.005*\"clinton\"\nScore: 0.10106444358825684\t Topic: 0.012*\"armenian\" + 0.010*\"israel\" + 0.008*\"isra\" + 0.006*\"turkish\" + 0.006*\"kill\"\nScore: 0.02939053624868393\t Topic: 0.005*\"pitt\" + 0.005*\"food\" + 0.004*\"bank\" + 0.004*\"medic\" + 0.004*\"research\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "category number: 14\n"
     ]
    }
   ],
   "source": [
    "cat_num = newsgroups_test.target[num]\n",
    "print(f'category number: {cat_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "category name: sci.space\n"
     ]
    }
   ],
   "source": [
    "print(f'category name: {newsgroups_test.target_names[cat_num]}')"
   ]
  }
 ]
}